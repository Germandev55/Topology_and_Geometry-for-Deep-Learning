{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Betti_manifold",
      "provenance": [],
      "authorship_tag": "ABX9TyOjXxTKgBnFLG7rHTuGr1fl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Germandev55/Topology_and_Geometry-for-Deep-Learning/blob/main/Betti_manifold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y4mQrAPTysv"
      },
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import IPython\n",
        "from IPython.display import HTML\n",
        "from google.colab import output\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Convolution2D, Input,Activation, ZeroPadding2D, MaxPooling2D, Flatten\n",
        "from keras import regularizers\n",
        "\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn import preprocessing\n",
        "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
        "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "\n",
        "!pip install ripser\n",
        "!pip install persim\n",
        "!pip install umap-learn\n",
        "from ripser import ripser\n",
        "from ripser import Rips\n",
        "from persim import plot_diagrams\n",
        "import umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNYU9VBbTzQG"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train = x_train * (1./255.)\n",
        "x_test = x_test * (1./255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9VLDHZsT03s"
      },
      "source": [
        "def features_to_X(x_test, model, index_layer, dim_red = 2):\n",
        "  input = x_test\n",
        "  layer = model.get_layer(index=index_layer) \n",
        "  layer_name = str(layer).split('.')[5].split(' ')[0]\n",
        "  features_layer1 = tf.keras.models.Model(inputs=model.inputs, outputs = layer.output)\n",
        "  output_ = features_layer1(input) \n",
        "  try:\n",
        "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    feature_batch_average = global_average_layer(output_)\n",
        "    X = feature_batch_average   \n",
        "  except:\n",
        "    X = output_   \n",
        "  return X, layer_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDTjEdDT2tN"
      },
      "source": [
        "def get_Betti(data, eps, maxdim=2):\n",
        "  Betti_layer = []\n",
        "  dgms = ripser(data, maxdim=maxdim, thresh=eps)['dgms']  \n",
        "  inf_in_list = lambda x: 1 if np.isinf(x).any() else 0\n",
        "  Betti_list = [0,0,0]\n",
        "  for e, i in enumerate(dgms):\n",
        "    for x in i: Betti_list[e] += inf_in_list(x)\n",
        "  return sum(Betti_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-oKgU1-T3jd"
      },
      "source": [
        "def get_Betti_prob(data, eps, maxdim=2):\n",
        "  dgms = ripser(data, maxdim=maxdim, thresh=eps)['dgms']  \n",
        "  prev_max = 0\n",
        "  list_Hom = []\n",
        "  mean_Hom = []\n",
        "  for Hom in dgms:\n",
        "    B_Hom = 0\n",
        "    for i in Hom:   \n",
        "      if np.isinf(i).any() == False:\n",
        "        B_Hom += (i[1] - i[0])\n",
        "        prev_max = i[1]\n",
        "      else:\n",
        "        continue \n",
        "        B_Hom += prev_max\n",
        "    list_Hom.append(B_Hom)\n",
        "    l = len(Hom)\n",
        "    if len(Hom) == 0: l=1\n",
        "    mean_Hom.append(B_Hom/l)\n",
        "\n",
        "  mean_Hom =  [round(x,3) for x in mean_Hom] \n",
        "  return sum(list_Hom), sum(mean_Hom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZDZixSUGRt"
      },
      "source": [
        "from sklearn.manifold import SpectralEmbedding\n",
        "def Betti_generaliz(models,eps_lft,examples):\n",
        "  models = models[0]\n",
        "  Global_Betti_prob = []\n",
        "  Global_Betti_mean = []\n",
        "  Global_Betti_sum = []\n",
        "\n",
        "  Betti_layer_listglobal = []\n",
        "  Betti_layer_mean_listglobal = []\n",
        "  sum_betti_listglobal = []\n",
        "\n",
        "  for model in models:\n",
        "    X_emb, layer_name = features_to_X(x_train[:examples], model, -2) \n",
        "    print(X_emb.shape[1])\n",
        "\n",
        "    Xtest = min_max_scaler.fit_transform(X_emb)\n",
        "    #Xtest = preprocessing.normalize(X_emb, norm='l1')\n",
        "    \n",
        "    #Xtest = max_abs_scaler.fit_transform(X_emb)\n",
        "\n",
        "    #embedding = LocallyLinearEmbedding(n_neighbors=120, n_components=10)\n",
        "    #Xtest  = embedding.fit_transform(Xtest)\n",
        "\n",
        "    #UMAP_reducer = umap.UMAP(n_components=7, min_dist=0.7, n_neighbors=1400, metric='euclidean') # 0.7/50/euclidean\n",
        "    #Xtest = UMAP_reducer.fit_transform(Xtest)\n",
        "\n",
        "    #Xtest = X_emb \n",
        "    print('Betti compute')\n",
        "    \n",
        "    Betti_layer_class = []\n",
        "    Betti_layer_mean_class = []\n",
        "\n",
        "    for c in list(range(10)):\n",
        "      X_emb_class = []\n",
        "      for e, i in enumerate(y_train[:examples]):\n",
        "        if i == c: X_emb_class.append(Xtest[e])\n",
        "      Xtest_class = np.array(X_emb_class)\n",
        "\n",
        "      Betti_layer, Betti_layer_mean = get_Betti_prob(Xtest_class, eps_lft, 2)\n",
        "\n",
        "      Betti_layer_class.append(Betti_layer)\n",
        "      Betti_layer_mean_class.append(Betti_layer_mean)\n",
        "\n",
        "    Betti_layer_listglobal.append(sum(Betti_layer_class)/10)\n",
        "    Betti_layer_mean_listglobal.append(sum(Betti_layer_mean_class)/10)  \n",
        "\n",
        "  Betti_layer_listglobal = [round(x,5) for x in  Betti_layer_listglobal]\n",
        "  Betti_layer_mean_listglobal = [round(x,5) for x in Betti_layer_mean_listglobal]\n",
        "  \n",
        "  return Betti_layer_listglobal, Betti_layer_mean_listglobal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW3_RbOJUXdV"
      },
      "source": [
        "examples = 4000\n",
        "eps_lft = np.inf\n",
        "Betti_local, Betti_global = Betti_generaliz(models_resnet, eps_lft, examples)\n",
        "print(Betti_local,Betti_global)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}